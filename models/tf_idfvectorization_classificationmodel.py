# -*- coding: utf-8 -*-
"""TF-IDFVectorization_ClassificationModel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JWFNjEDkVDF52sr8La9GgxjFLWTPMthZ
"""

import whisper

from sklearn.feature_extraction.text import TfidfVectorizer

# Step 1: Transcribe audio to text using Whisper
def transcribe_audio(audio_file):
    model = whisper.load_model("base")  # You can use 'small', 'medium', etc.
    result = model.transcribe(audio_file)
    return result['text']

# Step 2: Convert transcription to TF-IDF vector
def get_tfidf_vector(transcription, vectorizer=None):
    if vectorizer is None:
        vectorizer = TfidfVectorizer(max_features=50)  # Create one if not provided

    # For a single audio input, we still need to use a list
    tfidf_matrix = vectorizer.fit_transform([transcription])
    return tfidf_matrix.toarray(), vectorizer

from gtts import gTTS

text ="I have numbness in my hands and feet, and there are several white patches on my skin that don't have any feeling."
language = 'en'

tts = gTTS(text=text, lang=language, slow=False)

tts.save("symptom_description.mp3")

print("Audio file 'symptom_description.mp3' created successfully!")

# Path to your audio file (e.g., user recording)
audio_path = "/content/symptom_description.mp3"

# 1. Transcribe
transcription = transcribe_audio(audio_path)
print("Transcription:", transcription)

# 2. TF-IDF Vector
tfidf_vector, vectorizer = get_tfidf_vector(transcription)
print("TF-IDF Vector:", tfidf_vector)
print("TF-IDF Features:", vectorizer.get_feature_names_out())


from dotenv import load_dotenv
import os

load_dotenv()

GROQ_API_KEY = os.getenv("GROQ_API_KEY")



# === STEP 1: Imports and API Key Setup ===
import os
import json
import re
import pandas as pd
from groq import Groq
from IPython.display import FileLink


# === STEP 2: Extract JSON array ===
def extract_json_array(text):
    print("\nüîç RAW RESPONSE START:\n", text[:1000], "\nüîç RAW RESPONSE END\n")  # Print first 1000 chars for debug
    try:
        start = text.index("[")
        end = text.rindex("]") + 1
        json_str = text[start:end]
        return json.loads(json_str)
    except (ValueError, json.JSONDecodeError) as e:
        print("‚ùå Failed to extract/parse JSON:", e)
        return None

# === STEP 3: Generate dataset using Groq ===
def generate_dataset_groq(api_key, num_samples=100, output_file="leprosy_dataset.csv"):
    client = Groq(api_key=api_key)

    prompt = """
Generate 100 JSON examples of patient-reported symptom descriptions.
Each item should be like:
{"text": "<symptom description>", "label": 1 or 0},
where:
- 1 means it describes leprosy symptoms (e.g., white patches, numbness, lesions).
- 0 means it does NOT describe leprosy (e.g., clear skin, fever, cough, or no symptoms).

Be sure to include at least 50 examples of each. Include negation examples like 'no white patches' or 'skin is normal'.
Return ONLY a JSON array.
"""

    response = client.chat.completions.create(
        model="llama3-8b-8192",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7
    )

    raw = response.choices[0].message.content.strip()
    data = extract_json_array(raw)

    if not data:
        raise RuntimeError("‚ùå Failed to parse dataset JSON.")

    df = pd.DataFrame(data)
    df.to_csv(output_file, index=False)
    print(f"‚úÖ Dataset saved to {output_file}")
    return output_file

# === STEP 4: Run it ===
csv_path = generate_dataset_groq(GROQ_API_KEY, num_samples=100)

# === STEP 5: Provide download link ===
display(FileLink(csv_path))

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Step 2: Load your dataset
df = pd.read_csv("leprosy_dataset.csv")
print(df.head())

# Step 3: Prepare inputs and labels
X_text = df['text']       # Symptom descriptions
y = df['label']           # 1 = Leprosy-related, 0 = Not related

# Step 4: TF-IDF Vectorization
vectorizer = TfidfVectorizer(max_features=1000)
X_tfidf = vectorizer.fit_transform(X_text)

# Step 5: Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)

# Step 6: Train a Logistic Regression model
clf = LogisticRegression()
clf.fit(X_train, y_train)

# Step 7: Evaluate
y_pred = clf.predict(X_test)

print("\n‚úÖ Accuracy:", accuracy_score(y_test, y_pred))
print("\nüìä Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nüìã Classification Report:\n", classification_report(y_test, y_pred))

from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression

# Define model
model = LogisticRegression(max_iter=1000)

# Define parameter grid
param_grid = {
    'C': [0.01, 0.1, 1, 10],
    'penalty': ['l1', 'l2'],
    'solver': ['liblinear', 'saga']
}

# Create GridSearchCV object
grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy', verbose=1)

# Fit on training data
grid_search.fit(X_train, y_train)

# Best parameters and score
print("Best Parameters:", grid_search.best_params_)
print("Best Cross-Validation Accuracy:", grid_search.best_score_)

# Evaluate on test set
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
print("\nTest Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))

pip install scikit-learn joblib numpy

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer()
X = tfidf.fit_transform(df["text"])

joblib.dump(tfidf, "tfidf_vectorizer.pkl")

import joblib

# Save the best model from GridSearchCV
joblib.dump(grid_search.best_estimator_, "logistic_model.pkl")

# Save the fitted TF-IDF vectorizer (use correct name)
joblib.dump(tfidf, "tfidf_vectorizer.pkl")

import whisper
import joblib
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer

# === Load trained objects ===
# Load the trained model and TF-IDF vectorizer
model = joblib.load("logistic_model.pkl")          # Your tuned logistic regression model
vectorizer = joblib.load("tfidf_vectorizer.pkl")   # Your fitted TF-IDF vectorizer

# === Step 1: Transcribe the audio ===
def transcribe_audio(audio_path):
    model_whisper = whisper.load_model("base")
    result = model_whisper.transcribe(audio_path)
    print("üìù Transcription:", result['text'])
    return result['text']

# === Step 2: Preprocess and vectorize ===
def vectorize_text(text, vectorizer):
    return vectorizer.transform([text])

# === Step 3: Predict using trained model ===
def predict_leprosy(audio_path):
    # Step 1: Transcribe
    text = transcribe_audio(audio_path)

    # Step 2: TF-IDF vectorize
    tfidf_vector = vectorize_text(text, vectorizer)

    # Step 3: Predict
    prediction = model.predict(tfidf_vector)[0]
    probability = model.predict_proba(tfidf_vector)[0][prediction]

    print(f"üîÆ Prediction: {'Leprosy' if prediction == 1 else 'Not Leprosy'} (Confidence: {probability:.2f})")
    return prediction

# === Example Usage ===
# Replace 'sample.wav' with your actual file
predict_leprosy("/content/symptom_description.mp3")

print(df["label"].value_counts())

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from pytorch_tabnet.tab_model import TabNetClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import numpy as np
import torch

# Step 1: Load dataset
df = pd.read_csv("leprosy_dataset.csv")  # contains 'text' and 'label'

# Step 2: Split into train/test
X_train_text, X_test_text, y_train, y_test = train_test_split(
    df['text'], df['label'], test_size=0.2, random_state=42
)

# Step 3: TF-IDF vectorization
tfidf = TfidfVectorizer(max_features=500)
X_train = tfidf.fit_transform(X_train_text).toarray()
X_test = tfidf.transform(X_test_text).toarray()

# Step 4: Convert labels to int
y_train = np.array(y_train).astype(int)
y_test = np.array(y_test).astype(int)

# Step 5: Define TabNet model
tabnet = TabNetClassifier()
tabnet.fit(
    X_train=X_train, y_train=y_train,
    eval_set=[(X_test, y_test)],
    eval_name=["val"],
    eval_metric=["accuracy"],
    max_epochs=100,
    patience=10,
    batch_size=256,
    virtual_batch_size=128,
    num_workers=0,
    drop_last=False,
)

# Step 6: Evaluation
y_pred = tabnet.predict(X_test)
print("‚úÖ Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))

# Step 7: Save model + vectorizer
import joblib
tabnet.save_model("tabnet_leprosy")
joblib.dump(tfidf, "tfidf_vectorizer.pkl")
